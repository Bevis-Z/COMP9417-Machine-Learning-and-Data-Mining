{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ***Please use Kaggle Env to Load this notebook*** \n* https://www.kaggle.com/code/gambitwister/co-visitation-matrix-model\n# ***Please add the following dataset:***\n* https://www.kaggle.com/datasets/columbia2131/otto-chunk-data-inparquet-format","metadata":{}},{"cell_type":"markdown","source":"# Introduction\nThis notebook introduce how to built three Co-visitation matrix, the click/cart/order to cart/order matrix, the cart/order to cart-order matrix and click/cart/order to click matrix.\n\nOne thing to note is that this notebook uses the RAPIDs to speed up the calculation of the matrix.\n\nAnd the dataset is the chunk data in parquet format instead of the jsonl files. The aim for this is to decrease the size of the dataset and generate a more readable one.\n\nAs a result, several \".pqt\" files are generated as the output, which represent the 3 matrix.\n\nTo see the prediction part, please go to this notebook: https://www.kaggle.com/code/gambitwister/co-visitation-matrix-pred\n\nLet's start :)","metadata":{}},{"cell_type":"code","source":"import pandas as pd, numpy as np\nfrom tqdm.notebook import tqdm\nimport os, sys, pickle, glob, gc\nfrom collections import Counter\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2023-08-02T10:47:10.740794Z","iopub.execute_input":"2023-08-02T10:47:10.741095Z","iopub.status.idle":"2023-08-02T10:47:10.839363Z","shell.execute_reply.started":"2023-08-02T10:47:10.741070Z","shell.execute_reply":"2023-08-02T10:47:10.838507Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#### Use RAPIDs to speed up the Co-visitation Matrix calculation:\n#### Remember to use the GPU accelerator.","metadata":{}},{"cell_type":"code","source":"import cudf\ncudf.__version__","metadata":{"execution":{"iopub.status.busy":"2023-08-02T10:47:17.158751Z","iopub.execute_input":"2023-08-02T10:47:17.159105Z","iopub.status.idle":"2023-08-02T10:47:22.287190Z","shell.execute_reply.started":"2023-08-02T10:47:17.159077Z","shell.execute_reply":"2023-08-02T10:47:22.286217Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'23.06.01'"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Load the train and test files: \n##### modified from https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575/notebook","metadata":{}},{"cell_type":"code","source":"files = glob.glob('../input/otto-chunk-data-inparquet-format/*_parquet/*')\ntype_labels = {'clicks':0, 'carts':1, 'orders':2}\ndata_cache = {}\nfor f in files:\n    df = pd.read_parquet(f)\n    # change the 'ts' from ms to s\n    df.ts = (df.ts / 1000).astype('int32')\n    df['type'] = df['type'].map(type_labels).astype('int8')\n    data_cache[f] = df\n\n# INTERVAL means that there will be 5 files calculated at the same time\nINTERVAL = 5\n# CHUNK represents the amount of the files to process in one step\nCHUNK = int(np.ceil(len(files)/6))\n\ndef read_file(f):\n  return cudf.DataFrame(data_cache[f])\n\nDISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\ntype_weight = {0:1, 1:6, 2:3}","metadata":{"execution":{"iopub.status.busy":"2023-08-02T10:47:27.933207Z","iopub.execute_input":"2023-08-02T10:47:27.933583Z","iopub.status.idle":"2023-08-02T10:48:40.441435Z","shell.execute_reply.started":"2023-08-02T10:47:27.933552Z","shell.execute_reply":"2023-08-02T10:48:40.440414Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"The DISK_PIECES represents the number of disk parts, and SIZE is computed by the total amount of sessions divided by DISK_PIECES.","metadata":{}},{"cell_type":"markdown","source":"# click/cart/order to cart/order Co-visitation Matrix:\n* this part will generate 4 click_cart_order.pqt files","metadata":{}},{"cell_type":"code","source":"# there are 4 parts of the disks in total\nfor part in range(DISK_PIECES):\n  for step in range(6):\n    start_file_idx = step * CHUNK\n    end_file_idx = min( (step+1)*CHUNK, len(files) )\n\n    for idx in range(start_file_idx, end_file_idx, INTERVAL):\n      # load the first file of this interval into df\n      df = [read_file(files[idx])]\n      # load the rest files of this interval into df\n      for i in range(1, INTERVAL):\n        if idx+i < end_file_idx:\n          df.append(read_file(files[idx+1]))\n      df = cudf.concat(df,ignore_index=True,axis=0)\n      # sort the dataframe by the session-index (ascending) and ts (descending)\n      df = df.sort_values(['session','ts'],ascending=[True,False])\n      df = df.reset_index(drop=True)\n      df['n'] = df.groupby('session').cumcount()\n      # keep the sessions whose action-amount is less than 20\n      df = df.loc[df.n<20].drop('n',axis=1)\n\n      df = df.merge(df, on='session')\n      # keeps the sessions whose two actions are within 1 day\n      df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n      # control the aids to compute within the range of [part*SIZE, (part+1)*SIZE]\n      df = df.loc[(df.aid_x >= part*SIZE)&(df.aid_x < (part+1)*SIZE)]\n      # remove duplicate sessions\n      df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n\n      # assign type weights\n      df['weight'] = df.type_y.map(type_weight)\n\n      df = df[['aid_x','aid_y','weight']]\n      df.weight = df.weight.astype('float32')\n      df = df.groupby(['aid_x', 'aid_y']).weight.sum()\n      if idx==start_file_idx:\n        temp1 = df\n      else:\n        temp1 = temp1.add(df, fill_value=0)\n    # when the above for-loop breaks, temp1 stores 5 file df of this INTERVAL\n    # we need to store every temp1 into a new temp, and there are 6 temp1 in total\n    if start_file_idx == 0:\n      temp2 = temp1\n    else:\n      temp2 = temp2.add(temp1, fill_value=0)\n    del temp1, df\n    gc.collect()\n\n  # convert matrix to dictionary\n  temp2 = temp2.reset_index()\n  # sort the sessions based on the ascending aid and descending weight\n  temp2 = temp2.sort_values(['aid_x', 'weight'], ascending=[True, False])\n  temp2 = temp2.reset_index(drop=True)\n  # save the seesions whose action-amount is less than 15\n  temp2['n'] = temp2.groupby('aid_x').aid_y.cumcount()\n  temp2 = temp2.loc[temp2.n < 15].drop('n', axis=1)\n  # save matrix to the file carts_orders_{part}.pqt\n  temp2.to_pandas().to_parquet(f'clicks_carts_orders_{part}.pqt')","metadata":{"execution":{"iopub.status.busy":"2023-08-02T10:48:49.128630Z","iopub.execute_input":"2023-08-02T10:48:49.129138Z","iopub.status.idle":"2023-08-02T10:50:05.919661Z","shell.execute_reply.started":"2023-08-02T10:48:49.129100Z","shell.execute_reply":"2023-08-02T10:50:05.918599Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# cart/order to cart/order Co-visitation matrix:\n* this part will generate one cart_order.pqt file","metadata":{}},{"cell_type":"code","source":"DISK_PIECES = 1\nSIZE = 1.86e6/DISK_PIECES\n\n# there is one part of the disks in total\nfor part in range(DISK_PIECES):\n  for step in range(6):\n    start_file_idx = step * CHUNK\n    end_file_idx = min( (step+1)*CHUNK, len(files) )\n\n    for idx in range(start_file_idx, end_file_idx, INTERVAL):\n      # load the first file of this interval into df\n      df = [read_file(files[idx])]\n      # load the rest files of this interval into df\n      for i in range(1, INTERVAL):\n        if idx+i < end_file_idx:\n          df.append(read_file(files[idx+1]))\n      df = cudf.concat(df,ignore_index=True,axis=0)\n      # only keep the \"cart\" & \"order\" type\n      df = df.loc[df['type'].isin([1,2])]\n      # sort the dataframe by the session-index (ascending) and ts (descending)\n      df = df.sort_values(['session','ts'],ascending=[True,False])\n      df = df.reset_index(drop=True)\n      df['n'] = df.groupby('session').cumcount()\n      # keep the sessions whose action-amount is less than 20\n      df = df.loc[df.n<20].drop('n',axis=1)\n\n      df = df.merge(df, on='session')\n      # keeps the sessions whose two actions are within 14 day\n      df = df.loc[((df.ts_x - df.ts_y).abs()< 14 * 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n      # control the aids to compute within the range of [part*SIZE, (part+1)*SIZE]\n      df = df.loc[(df.aid_x >= part*SIZE)&(df.aid_x < (part+1)*SIZE)]\n      # remove duplicate sessions\n      df = df[['session', 'aid_x', 'aid_y','type_y']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n\n      # assign type weights\n      df['weight'] = 1\n\n      df = df[['aid_x','aid_y','weight']]\n      df.weight = df.weight.astype('float32')\n      df = df.groupby(['aid_x', 'aid_y']).weight.sum()\n      if idx==start_file_idx:\n        temp1 = df\n      else:\n        temp1 = temp1.add(df, fill_value=0)\n    # when the above for-loop breaks, temp1 stores 5 file df of this INTERVAL\n    # we need to store every temp1 into a new temp, and there are 6 temp1 in total\n    if start_file_idx == 0:\n      temp2 = temp1\n    else:\n      temp2 = temp2.add(temp1, fill_value=0)\n    del temp1, df\n    gc.collect()\n\n  # convert matrix to dictionary\n  temp2 = temp2.reset_index()\n  # sort the sessions based on the ascending aid and descending weight\n  temp2 = temp2.sort_values(['aid_x', 'weight'], ascending=[True, False])\n  temp2 = temp2.reset_index(drop=True)\n  # save the seesions whose action-amount is less than 15\n  temp2['n'] = temp2.groupby('aid_x').aid_y.cumcount()\n  temp2 = temp2.loc[temp2.n < 15].drop('n', axis=1)\n  # save matrix to the file carts_orders_{part}.pqt\n  temp2.to_pandas().to_parquet(f'carts_orders_{part}.pqt')","metadata":{"execution":{"iopub.status.busy":"2023-08-02T10:50:17.989896Z","iopub.execute_input":"2023-08-02T10:50:17.990359Z","iopub.status.idle":"2023-08-02T10:50:31.851369Z","shell.execute_reply.started":"2023-08-02T10:50:17.990317Z","shell.execute_reply":"2023-08-02T10:50:31.850356Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# click/cart/order to click Co-visitation matrix:\n* thie part will generate 4 click.pqt files","metadata":{}},{"cell_type":"code","source":"DISK_PIECES = 4\nSIZE = 1.86e6/DISK_PIECES\n\n# there are 4 parts of the disks in total\nfor part in range(DISK_PIECES):\n  for step in range(6):\n    start_file_idx = step * CHUNK\n    end_file_idx = min( (step+1)*CHUNK, len(files) )\n\n    for idx in range(start_file_idx, end_file_idx, INTERVAL):\n      # load the first file of this interval into df\n      df = [read_file(files[idx])]\n      # load the rest files of this interval into df\n      for i in range(1, INTERVAL):\n        if idx+i < end_file_idx:\n          df.append(read_file(files[idx+1]))\n      df = cudf.concat(df,ignore_index=True,axis=0)\n      # sort the dataframe by the session-index (ascending) and ts (descending)\n      df = df.sort_values(['session','ts'],ascending=[True,False])\n      df = df.reset_index(drop=True)\n      df['n'] = df.groupby('session').cumcount()\n      # keep the sessions whose action-amount is less than 20\n      df = df.loc[df.n<20].drop('n',axis=1)\n\n      df = df.merge(df, on='session')\n      # keeps the sessions whose two actions are within 1 day\n      df = df.loc[((df.ts_x - df.ts_y).abs()< 24 * 60 * 60) & (df.aid_x != df.aid_y)]\n      # control the aids to compute within the range of [part*SIZE, (part+1)*SIZE]\n      df = df.loc[(df.aid_x >= part*SIZE)&(df.aid_x < (part+1)*SIZE)]\n      # remove duplicate sessions\n      df = df[['session', 'aid_x', 'aid_y','ts_x']].drop_duplicates(['session', 'aid_x', 'aid_y'])\n\n      # assign type weights\n      df['weight'] = 1 + 3*(df.ts_x - 1659304800)/(1662328791-1659304800)\n\n      df = df[['aid_x','aid_y','weight']]\n      df.weight = df.weight.astype('float32')\n      df = df.groupby(['aid_x', 'aid_y']).weight.sum()\n      if idx==start_file_idx:\n        temp1 = df\n      else:\n        temp1 = temp1.add(df, fill_value=0)\n    # when the above for-loop breaks, temp1 stores 5 file df of this INTERVAL\n    # we need to store every temp1 into a new temp, and there are 6 temp1 in total\n    if start_file_idx == 0:\n      temp2 = temp1\n    else:\n      temp2 = temp2.add(temp1, fill_value=0)\n    del temp1, df\n    gc.collect()\n\n  # convert matrix to dictionary\n  temp2 = temp2.reset_index()\n  # sort the sessions based on the ascending aid and descending weight\n  temp2 = temp2.sort_values(['aid_x', 'weight'], ascending=[True, False])\n  temp2 = temp2.reset_index(drop=True)\n  # save the seesions whose action-amount is less than 15\n  temp2['n'] = temp2.groupby('aid_x').aid_y.cumcount()\n  temp2 = temp2.loc[temp2.n < 20].drop('n', axis=1)\n  # save matrix to the file carts_orders_{part}.pqt\n  temp2.to_pandas().to_parquet(f'clicks_{part}.pqt')","metadata":{"execution":{"iopub.status.busy":"2023-08-02T10:51:23.684863Z","iopub.execute_input":"2023-08-02T10:51:23.685270Z","iopub.status.idle":"2023-08-02T10:52:37.419975Z","shell.execute_reply.started":"2023-08-02T10:51:23.685239Z","shell.execute_reply":"2023-08-02T10:52:37.418730Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"del data_cache, temp2\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-08-02T09:47:30.105419Z","iopub.execute_input":"2023-08-02T09:47:30.105773Z","iopub.status.idle":"2023-08-02T09:47:30.278019Z","shell.execute_reply.started":"2023-08-02T09:47:30.105744Z","shell.execute_reply":"2023-08-02T09:47:30.276722Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"52"},"metadata":{}}]},{"cell_type":"markdown","source":"# Finally, I've uploaded the output files as a data set in kaggle:\nhttps://www.kaggle.com/datasets/gambitwister/co-visitation-matrix-9417\n\n# Please go to this notebook to see the prediction part:\nhttps://www.kaggle.com/code/gambitwister/co-visitation-matrix-pred","metadata":{}}]}